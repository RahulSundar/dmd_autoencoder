{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Fluid Flow slow manifold Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the nonlinear mean-field model of fluid flow past a circular cylinder at Reynolds number 100, described by empirical Galerkin model:\n",
    "$$\n",
    "\\frac{\\partial x_{1}}{\\partial t} = \\mu x_{1} - \\omega x_{2} + A x_{1}x_{3}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial x_{2}}{\\partial t} = \\omega x_{1} + \\mu x_{2} + Ax_{2}x_{3}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial x_{3}}{\\partial t} = -\\lambda(x_{3} - {x_{1}}^{2} - {x_{2}}^{2}) \n",
    "$$\n",
    "\n",
    "Where $\\mu = 0.1, \\omega = 1, A=-0.1, \\lambda = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from dmd_machine.dmd_ae_machine import DMDMachine\n",
    "from dmd_machine.loss_function import LossFunction\n",
    "from data.Data import DataMaker\n",
    "from datetime import date \n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from return_stats import *\n",
    "from create_plots import *\n",
    "from datetime import date  \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Read in dataset.\n",
    "# ======================================================================================================================\n",
    "\n",
    "training_data = pickle.load(open('./data/dataset_fluid.pkl', 'rb'))\n",
    "\n",
    "input_data = training_data.data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper Parameters.\n",
    "hyp_params = dict()\n",
    "hyp_params['num_t_steps'] = training_data.params['num_time_steps']\n",
    "hyp_params['phys_dim'] = training_data.params[\"num_physical_dim\"]\n",
    "hyp_params['num_init_conds'] = training_data.params['num_initial_conditions']\n",
    "hyp_params['batch_size'] = 256\n",
    "hyp_params['num_epochs'] = 200\n",
    "\n",
    "# Encoding/Decoding Layer Parameters.\n",
    "hyp_params['num_en_layers'] = 3\n",
    "hyp_params['num_en_neurons'] = 80\n",
    "hyp_params['latent_dim'] = 3\n",
    "hyp_params['window_size'] = 256\n",
    "\n",
    "hyp_params['activation'] = 'elu'\n",
    "hyp_params['weight_initializer'] = 'he_uniform'\n",
    "hyp_params['bias_initializer'] = 'he_uniform'\n",
    "hyp_params['ae_output_activation'] = \"linear\"\n",
    "hyp_params['hidden_activation'] = \"elu\"\n",
    "\n",
    "hyp_params['c1'] = 1  # coefficient auto-encoder loss.\n",
    "hyp_params['c2'] = 1  # coefficient of dmd loss.\n",
    "hyp_params['c3'] = 1  # coefficient of pred loss.\n",
    "\n",
    "# save results in the folder \" Results/save_folder\"- including loss curves and plot latent data.\n",
    "save_folder = \"AeEx3_\" + str(date.today().isoformat()) \n",
    "\n",
    "# number of initial conditions in training and testing dataset.\n",
    "hyp_params['num_init_conds_training'] = int(0.8 * hyp_params['num_init_conds'])\n",
    "hyp_params['num_init_conds_test'] = hyp_params['num_init_conds'] - hyp_params['num_init_conds_training']\n",
    "\n",
    "# initialize machine and loss objects.\n",
    "myMachine = DMDMachine(hyp_params)\n",
    "# myMachine.autoencoder = keras.models.load_model(\"./models/my_model_Ex2_oct21\", compile=False)\n",
    "myLoss = LossFunction(hyp_params)\n",
    "\n",
    "# Learning rate initialization.\n",
    "hyp_params[\"initial_learning_rate\"] = 3e-3  # MAJOR PARAMETER CHOICE\n",
    "hyp_params[\"esteps\"] = 30  # MAJOR PARAMETER CHOICE\n",
    "count = 0\n",
    "\n",
    "# clear previous run session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# create folder to save results.\n",
    "create_new_folders(save_folder)\n",
    "\n",
    "# save hyperparams in a json file.\n",
    "save_hyp_params_in_json(hyp_params=hyp_params, json_file_path=os.path.join(\"results\", save_folder, \"hyp_params.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of training dataset (ic x phys_dim x timesteps) =  (8000, 3, 121)\n",
      "dimensions of testing dataset (ic x phys_dim x timesteps) =  (2000, 3, 121)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# Prepare dataset. \n",
    "# ======================================================================================================================\n",
    "# shuffle the dataset and then divide to training vs testing data sets. 80% training .20% testing.\n",
    "data_train, data_test= train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"dimensions of training dataset (ic x phys_dim x timesteps) = \", np.shape(data_train))\n",
    "print(\"dimensions of testing dataset (ic x phys_dim x timesteps) = \", np.shape(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Unit test to verify that testing and training datasets are disjoint.\n",
    "# ======================================================================================================================\n",
    "for ic_train in data_train:\n",
    "    for ic_test in data_test:\n",
    "        if ic_test[:, 0][0] == ic_train[:, 0][0] and ic_test[:, 0][1] == ic_train[:, 0][1]\\\n",
    "        and ic_test[:, 0][2] == ic_train[:, 0][2]:\n",
    "            print(\"Testing and Training datasets intersect!\")\n",
    "            print(ic_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datasets from numpy to tensorflow.\n",
    "data_train =  tf.data.Dataset.from_tensor_slices(data_train)\n",
    "data_test =  tf.data.Dataset.from_tensor_slices(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (3, 121), types: tf.float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
